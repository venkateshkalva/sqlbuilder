#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
from llama_index.indices.loading import load_index_from_storage
from llama_index.storage.storage_context import StorageContext

from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTVectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
import sys
import os
import openai
from IPython.display import Markdown, display

def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_output = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600
    chunk_overlap_ratio = 0.5  # A float value between 0.0 and 1.0
    openai.api_key = "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"

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", max_tokens=num_output))

    #define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_output,chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)

    documents = SimpleDirectoryReader(directory_path).load_data()
    service_context = ServiceContext.from_defaults(llm_predictor,prompt_helper=prompt_helper)

    index = GPTVectorStoreIndex.from_documents(
        documents, service_context= service_context
    )
    index.storage_context.persist(persist_dir="index.json")
    #index.save_to_disk('index.json')
    index.set_index_id('index.json')
    return index

def ask_ai():
    storage_context = StorageContext.from_defaults(persist_dir="index.json")
    index = load_index_from_storage(storage_context)
    query_engine = index.as_query_engine()
    while True:
        query = input("Ask AI: ")
        response = query_engine.query(query)
        display(Markdown(f"Response: <b>{response.response}</b>"))
